blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "fc_2.w_0.opti.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
          dims: 2
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_0.b_0.opti.trainer_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_0.b_0.opti.trainer_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.b_0.opti.trainer_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_2.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 2
        }
        lod_level: 0
      }
    }
    persistable: true
  }
  vars {
    name: "fc_2.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
          dims: 2
        }
        lod_level: 0
      }
    }
    persistable: true
  }
  vars {
    name: "fc_0.b_0.opti.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 15
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
  }
  vars {
    name: "fc_0.w_0.opti.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 15
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_0.w_0.opti.trainer_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 15
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
  }
  vars {
    name: "fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
  }
  vars {
    name: "fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
  }
  vars {
    name: "fc_2.b_0.opti.trainer_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 2
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_2.w_0.opti.trainer_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
          dims: 2
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_0.w_0.opti.trainer_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 15
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.b_0.opti.trainer_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.w_0.opti.trainer_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
          dims: 128
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.w_0.opti.trainer_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
          dims: 128
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.w_0.opti.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
          dims: 128
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.b_0.opti.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_2.b_0.opti.trainer_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 2
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_2.b_0.opti.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 2
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_2.w_0.opti.trainer_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
          dims: 2
        }
      }
    }
    persistable: false
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_0.b_0.opti.trainer_0"
      arguments: "fc_0.b_0.opti.trainer_1"
      arguments: "fc_0.b_0.opti.trainer_2"
      arguments: "fc_0.w_0.opti.trainer_0"
      arguments: "fc_0.w_0.opti.trainer_1"
      arguments: "fc_0.w_0.opti.trainer_2"
      arguments: "fc_1.b_0.opti.trainer_0"
      arguments: "fc_1.b_0.opti.trainer_1"
      arguments: "fc_1.b_0.opti.trainer_2"
      arguments: "fc_1.w_0.opti.trainer_0"
      arguments: "fc_1.w_0.opti.trainer_1"
      arguments: "fc_1.w_0.opti.trainer_2"
      arguments: "fc_2.b_0.opti.trainer_0"
      arguments: "fc_2.b_0.opti.trainer_1"
      arguments: "fc_2.b_0.opti.trainer_2"
      arguments: "fc_2.w_0.opti.trainer_0"
      arguments: "fc_2.w_0.opti.trainer_1"
      arguments: "fc_2.w_0.opti.trainer_2"
    }
    type: "fl_listen_and_serv"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 496, in get_pserver_program\n    attrs=attrs)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "optimize_blocks"
      type: BLOCKS
      blocks_idx: 1
      blocks_idx: 2
      blocks_idx: 3
      blocks_idx: 4
      blocks_idx: 5
      blocks_idx: 6
    }
    attrs {
      name: "Fanin"
      type: INT
      i: 3
    }
    attrs {
      name: "sync_mode"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "endpoint"
      type: STRING
      s: "127.0.0.1:8181"
    }
  }
}
blocks {
  idx: 1
  parent_idx: 0
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_0.b_0.opti.trainer_0"
      arguments: "fc_0.b_0.opti.trainer_1"
      arguments: "fc_0.b_0.opti.trainer_2"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.b_0"
    }
    type: "sum"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 475, in get_pserver_program\n    attrs={\"use_mkldnn\": False})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.b_0"
    }
    type: "scale"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 480, in get_pserver_program\n    attrs={\"scale\": 1.0 / float(self.trainer_num)})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.3333333432674408
    }
  }
}
blocks {
  idx: 2
  parent_idx: 0
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_0.w_0.opti.trainer_0"
      arguments: "fc_0.w_0.opti.trainer_1"
      arguments: "fc_0.w_0.opti.trainer_2"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.w_0"
    }
    type: "sum"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 475, in get_pserver_program\n    attrs={\"use_mkldnn\": False})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.w_0"
    }
    type: "scale"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 480, in get_pserver_program\n    attrs={\"scale\": 1.0 / float(self.trainer_num)})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.3333333432674408
    }
  }
}
blocks {
  idx: 3
  parent_idx: 0
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_1.b_0.opti.trainer_0"
      arguments: "fc_1.b_0.opti.trainer_1"
      arguments: "fc_1.b_0.opti.trainer_2"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.b_0"
    }
    type: "sum"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 475, in get_pserver_program\n    attrs={\"use_mkldnn\": False})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.b_0"
    }
    type: "scale"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 480, in get_pserver_program\n    attrs={\"scale\": 1.0 / float(self.trainer_num)})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.3333333432674408
    }
  }
}
blocks {
  idx: 4
  parent_idx: 0
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_1.w_0.opti.trainer_0"
      arguments: "fc_1.w_0.opti.trainer_1"
      arguments: "fc_1.w_0.opti.trainer_2"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.w_0"
    }
    type: "sum"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 475, in get_pserver_program\n    attrs={\"use_mkldnn\": False})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.w_0"
    }
    type: "scale"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 480, in get_pserver_program\n    attrs={\"scale\": 1.0 / float(self.trainer_num)})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.3333333432674408
    }
  }
}
blocks {
  idx: 5
  parent_idx: 0
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_2.b_0.opti.trainer_0"
      arguments: "fc_2.b_0.opti.trainer_1"
      arguments: "fc_2.b_0.opti.trainer_2"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_2.b_0"
    }
    type: "sum"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 475, in get_pserver_program\n    attrs={\"use_mkldnn\": False})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "fc_2.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_2.b_0"
    }
    type: "scale"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 480, in get_pserver_program\n    attrs={\"scale\": 1.0 / float(self.trainer_num)})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.3333333432674408
    }
  }
}
blocks {
  idx: 6
  parent_idx: 0
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_2.w_0.opti.trainer_0"
      arguments: "fc_2.w_0.opti.trainer_1"
      arguments: "fc_2.w_0.opti.trainer_2"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_2.w_0"
    }
    type: "sum"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 475, in get_pserver_program\n    attrs={\"use_mkldnn\": False})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "fc_2.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_2.w_0"
    }
    type: "scale"
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2610, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_distribute_transpiler.py\", line 480, in get_pserver_program\n    attrs={\"scale\": 1.0 / float(self.trainer_num)})\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/strategy/fl_strategy_base.py\", line 281, in _build_server_programs_for_job\n    main_prog = transpiler.get_pserver_program(endpoint)\n"
      strings: "  File \"/home/beiyu/anaconda3/envs/paddle/lib/python3.6/site-packages/paddle_fl/paddle_fl/core/master/job_generator.py\", line 152, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"/home/beiyu/PycharmProjects/pythonProject/gui/serverControlFrame.py\", line 135, in init_env\n    strategy, server_endpoints=endpoints, worker_num=int(self.config[\'parameter\'][\'num_users\']), output=output)\n"
      strings: "  File \"serverFrame.py\", line 73, in <module>\n    sys.exit(app.exec_())\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.3333333432674408
    }
  }
}
version {
  version: 1008004
}
